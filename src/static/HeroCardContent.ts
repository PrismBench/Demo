import BenchmarkComparison from "../components/common/HeroCard/BenchmarkComparison";
import StaticBenchmarkSaturation from "../components/graphics/StaticBenchmarkSaturation";
import LLMCapabilityEvolution from "../components/graphics/LLMCapabilityEvolution";
import NaiveDynamicBenchmark from "../components/graphics/NaiveDynamicBenchmark";
import EvaluationAdaption from "../components/graphics/EvaluationAdaption";
import MultiAgentEval from "../components/graphics/MultiAgentEval";
export const HeroCardContent = {
  title: "Rethinking LLM Evaluation: Why Static Benchmarks Fall Short",
  description:
    "Static benchmarks lose diagnostic value as models improve: they saturate early, fail to capture new failure modes, and reward superficial heuristics. As model behaviors evolve, evaluation must adapt. We propose a dynamic framework that builds branching task trajectories based on model performance, using multi-agent coordination and reinforcement learning to uncover weaknesses and refine evaluation pressure.",
  expandButtonText: "Why LLM Evaluation Must Adapt",
  collapseButtonText: "Collapse",
  heroDiagram: BenchmarkComparison,
  sections: [
    {
      title: "Static Benchmarks Saturate Quickly",
      description:
        "Most evaluation suites consist of a fixed set of tasks [1], [2] drawn from a precompiled dataset. Once a model performs well on these, the benchmark ceases to reveal meaningful differences. Over time, models become overfitted to the test set distribution, and further improvements in capability are not captured which masks important weaknesses behind perfect or near-perfect scores.",
      diagram: StaticBenchmarkSaturation,
      citations: [
        {
          content:
            "test",
        },
        {
          content: "Popover 2 content",
        },
      ],
    },
    {
      title: "LLM Capabilities Are Evolving Rapidly",
      description:
        "Recent improvements in model scale, architecture, and optimization have led to significant gains in performance across a range of tasks. These gains are often nonlinear and driven by discrete changes such as increased parameter counts, improved training objectives, or techniques like reasoning. However, benchmark difficulty has remained largely fixed. As a result, measured performance increases even when the underlying evaluation signal remains unchanged, limiting our ability to assess whether models are genuinely improving or merely exploiting static benchmarks.",
      diagram: LLMCapabilityEvolution,
    },
    {
      title: "Evaluation Frameworks Must Adapt",
      description:
        "LLMs often learn to exploit artifacts in training or test data. Which results in shortcuts that yield high scores without true generalization. Static evaluation frameworks cannot keep pace with these evolving behaviors. To remain meaningful, evaluation must become conditional: adapting to the model's responses and searching for the precise boundary between success and failure and the root cause of failure.",
      diagram: EvaluationAdaption,
    },
    {
      title: "Why Naive Dynamic Benchmarks Still Fall Short",
      description:
        "Some recent benchmarks attempt dynamism by adding new tasks or adversarial examples. However, these are often generated by other LLMs or pulled from static banks of prompts. When generation and evaluation are both handled by models with shared blind spots, the result is an echo chamber: familiar failures go unnoticed, and benchmarks overestimate robustness.",
      diagram: NaiveDynamicBenchmark,
    },
    {
      title: "Multi-Agent, Model-Agnostic Evaluation",
      description:
        "An effective dynamic benchmark must operate as a system: generating tasks, analyzing failures, and adjusting difficulty based on observed behavior. This requires coordinated search, often with multiple agents proposing tasks, identifying error types, or probing specific capabilities. Crucially, scoring must not rely on LLMs themselves. Instead, reward signals should come from deterministic, model-agnostic criteria such as unit test pass rates, symbolic correctness, or trusted human annotation.",
      diagram: MultiAgentEval,
    },
    {
      title: "Prism: A Framework for Dynamic, Verifiable Evaluation",
      description:
        "Prism evaluates LLMs through a branching task tree that evolves based on model behavior. Each branch isolates a different failure mode or reasoning path. Multiple agents explore the space of possible probes, and scoring is grounded in external oracles and never in the models themselves. As a result, Prism produces evaluations that stay diagnostic over time, surface failure clusters, and resist optimization through shortcut learning.",
      diagram: "",
    },
  ],
};
